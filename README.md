# üöó Clusteriza√ß√£o de carros com PySpark: Uma viagem com Aprendizados Reais

Durante a P√≥s-Gradua√ß√£o um dos meus professores fez uma apresenta√ß√£o incrivel mostrando como era feito a clusteriza√ß√£o de dados utilizando Apache Spark, eu fiquei maravilhado, mas n√£o tive a oportunidade de desenvolver a ideia aprensentada na pr√°tica. Depois que tive algumas orienta√ß√µes e concluir alguns cursos pelo programa ONE, eu me senti capaz e fiquei convicto que poderia tentar replicar o que o professor fez. No inicio do ano eu tinha conseguido tal feito, mas n√£o estava satisfeito, muitas quest√µes me vieram a mente: "E se?", "Por que isso?", "Tem outra forma?", "essa √© a melhor forma?", "Como isso √© feito?", enfim, hoje eu conclui e apresento a voc√™s o que eu fiz al√©m do que me foi apresentado.

Em resumo o Apache Spark √© uma poderosa ferramenta para processamento distribuido. E esse projeto tem como objetivo explorar dados de autom√≥veis e agrupalos em clusters com base em caracteristicas como conumo, pot√™ncia e tipo de carro.

Abaixo est√° a estrutura do DataFrame:

![Schema do Dataframe](https://github.com/user-attachments/assets/c67800f9-11fd-4be7-9cc1-8cb3504509b4)

E antes de come√ßar √© bom visualizar as 5 primeiras linhas do Dataframe:
![Top 5 das linhas do DF](https://github.com/user-attachments/assets/6c3ba80d-035e-41f4-bc19-a0d168a6407f)


## ‚öôÔ∏è Vers√£o Inicial ‚Äî Simplicidade e Controle Manual
Essa √© a vers√£o que me foi apresentado em aula, onde o foco estava em mostrar cada etapa do processo. Foi √≥timo, pois nos d√° controle do processo controle, todavia tornou o c√≥digo mais extenso e dif√≠cil de escalar.

Nessa vers√£o, todo o pr√©-processamento foi feito "a m√£o" como:
> üõ†Ô∏è Conver√ß√£o de vari√°veis categ√≥ricas:
<p>De forma simplificada, para os casos em que os itens no dataframe correspondia aos que estavam em List (Lista de pontas ou de tipo de carro), era atribuido ao valor da categoria; sua posi√ß√£o na "List" mais 1. Quando n√£o correspondia era atribuido o valor "2.0". E isso era feito para cada coluna em "Colunas".</p>

~~~python
def transform (coluna,Df,List):
    for item in List:
        i = List.index(item) + 1
        Df = Df.withColumn(coluna, when(Df[coluna] == item, float(i) ).otherwise(2.0))
        
    return Df

# Percorrer entre colunas e listas de valores unicos
Colunas = ["portas","tipo"]
Listas = [portas,tipos]
for i in range(0,len(Colunas)):
    CarrosDF2 = transform(Colunas[i],CarrosDF2,Listas[i])
~~~
Um dos motivos de eu desenvolver mudan√ßas no c√≥digo √© devido ao caso anterior. A fun√ß√£o percorre a coluna do Dataframe inteira para todo o "Item" em "List", al√©m de ser ineficiente sobrescreve os valores da coluna a cada nova intera√ß√£o. A solu√ß√£o que pensei inicialmente era retirar as linhas que correspondiam aos itens da lista e depois adicion√°-los ao dataframe principal. A ideia era s√≥ realizar a transforma√ß√£o nos casos em que havia correspond√™ncia e depois de percorrido a "List" realizar a transforma√ß√£o nos casos que n√£o correspondia. Pensei tamb√©m na possibilidade apenas listar a posi√ß√£o de cada linha que n√£o correspondia e ir percorrendo do Dataframe atrav√©s dessas posi√ß√µes em uma lista por exemplo, e ir retirando a cada nova correspond√™ncia, no fim, teria uma lista com a posi√ß√£o de cada linha que n√£o houve correspond√™ncia com "List" e assim seria feito a transforma√ß√£o nesses casos. Esta ultima alternativa seria prefer√≠vel ao meu ver.
> üéõÔ∏è Normaliza√ß√£o estat√≠stica manual usando m√©dias e desvios:
<p>Aproveitando o ".describe()" do pandas e convertendo-o para dataframe, foi possivel gerar as m√©dias e desvio padr√£o para as colunas analisadas. A normaliza√ß√£o foi feita para cada "i" em um intervalo de 0 ao tamanho da lista de m√©dias calculadas.</p>

~~~python
#üíâExtraindo a m√©dia e o desvio padr√£o
medias = descricaoNova.iloc[1,1:6].values.tolist()
desvios = descricaoNova.iloc[2,1:6].values.tolist()
~~~

~~~python
# Fun√ß√£o para Normalizar e criar os vetores densos
def centerAndScale(inRow):
  #Vari√°veis Globais
  global bc_media
  global bc_desvio
  #Array de m√©dias e desvios
  meanArray = bc_media.value
  stdArray = bc_desvio.value
  #Array para o resultados
  retArray= []
for i in range(len(meanArray)):
    retArray.append((float(inRow[i])-float(meanArray[i]))/float(stdArray[i])) # üéõÔ∏è Normaliza√ß√£o
  return Row(features = Vectors.dense(retArray)) # ü™Ñ Cria√ß√£o de vetores densos com Vectors.dense:
~~~
## üîÅ Mudan√ßa de Rota ‚Äî Explorando o Pipeline do Spark MLlib

Percebi que poderia melhorar a robustez e modularidade do c√≥digo utilizando o Pipeline do Spark MLlib.

~~~python
pipeline = Pipeline(stages=[indexer_portas, indexer_tipo, encoder, assembler, scaler])
~~~

Ent√£o Implementei:

* StringIndexer + OneHotEncoder para tratar vari√°veis categ√≥ricas:
~~~python
indexer_portas = StringIndexer(inputCol="portas", outputCol="portas_indexadas")
indexer_tipo = StringIndexer(inputCol="tipo", outputCol="tipo_indexado")
~~~
A classe StringIndexer converte colunas categ√≥ricas em √≠ndices num√©ricos, atribuindo um n√∫mero inteiro a cada categoria.
O √≠ndice atribu√≠do √© baseado na frequ√™ncia da categoria, onde a categoria mais frequente recebe o √≠ndice 0, a segunda mais frequente recebe o √≠ndice 1 e assim por diante.
* VectorAssembler para consolidar as features:

~~~python
assembler = VectorAssembler(inputCols=["portas_encoded", "tipo_encoded", "horsepowerf", "rpmf", "consumo_cidadef"], outputCol="features_nao_escaladas")
~~~

O VectorAssembler combina v√°rias colunas em uma √∫nica coluna de vetor.
O par√¢metro inputCols especifica as colunas de entrada que ser√£o combinadas, e o par√¢metro outputCol especifica o nome da coluna de sa√≠da que conter√° o vetor resultante.
O par√¢metro handleInvalid="skip" √© usado para lidar com valores inv√°lidos durante a transforma√ß√£o. Nesse caso, os valores inv√°lidos ser√£o ignorados e n√£o inclu√≠dos no vetor resultante.

* StandardScaler para normaliza√ß√£o:
  
~~~python
scaler = StandardScaler(inputCol="features_nao_escaladas", outputCol="features", withStd=True, withMean=True)
~~~

O StandardScaler √© usado para padronizar as features, ou seja, ajustar a m√©dia e o desvio padr√£o de cada feature para que tenham m√©dia 0 e desvio padr√£o 1.
O par√¢metro withStd=True indica que o escalonamento deve ser feito com base no desvio padr√£o, enquanto withMean=True indica que a m√©dia deve ser subtra√≠da.
Isso √© √∫til para garantir que todas as features tenham a mesma escala e contribuam igualmente para o modelo de aprendizado de m√°quina.

* KMeans para clusteriza√ß√£o:
~~~python
# Criando modelo
kmeans = KMeans(k=4, seed=1)
# Treinando o modelo
modelo = kmeans.fit(carrosDfPreparado.select("features"))
~~~

O modelo √© treinado usando o m√©todo fit, que ajusta o modelo aos dados de entrada. O resultado √© um modelo KMeans treinado que pode ser usado para prever os clusters dos dados.
O modelo KMeans √© um algoritmo de aprendizado n√£o supervisionado que agrupa os dados em k clusters com base nas caracter√≠sticas fornecidas.
O par√¢metro seed √© usado para garantir a reprodutibilidade dos resultados.

> *Essa mudan√ßa exigiu aprender algumas novas pr√°ticas, mas deixou o projeto preparado para datasets maiores.*

## Resultado:
O gr√°fico abaixo mostra a distribui√ß√£o dos veiculos em fun√ß√£o do "Housepower" e "Concumo_cidade". O modelo identificou 5 grupos distintos de carros que possuem perfis semelhante, e s√£o representado por cores diferentes no gr√°fico. Esse agrupamento (segmenta√ß√£o) considerou vari√°veis como pot√™ncia, consumo, tipo do carro e n√∫mero de portas.


![Gr√°fico de Dispers√£o](https://github.com/user-attachments/assets/0a93ef07-3909-41c4-91ea-7c9a74755fee)


Rela√ß√£o de Vaiculos por Clusters:
![Gr√°fico de Barras](https://github.com/user-attachments/assets/66ecc819-c0f4-4f15-b2fd-82b638529d63)


## üìä Pr√≥ximos passos
Agora que a estrutura est√° bem definida, pretendo:

* Tentar caracterizar os clusters realizando "groupBy" dos dados pela predi√ß√£o ou por outra forma;
* Aplicar o modelo em datasets maiores;
* Explorar outros m√©todos de avalia√ß√£o al√©m da silhueta;
* Testar pipelines com outras t√©cnicas de normaliza√ß√£o e redu√ß√£o de dimensionalidade (como An√°lise de Componentes Principais - PCA);

## ‚úçÔ∏è Publicado por:
Ailton J√∫nior ‚Äì Estudante em [Gest√£o de Big Data e Anlise de negocios]
#Spark #BigData #MachineLearning #DataScience #Python #Clusteriza√ß√£o #Storytelling
